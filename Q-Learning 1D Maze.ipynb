{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Maze with Q Learning\n",
    "\n",
    "In this notebook we attempt to solve the 1D Maze reinforcement learning problem. However, instead of using _TD Learning_ we will be using _Q Learning_. The difference is in the value function and the delta function. When using Q learning, one considers the value of a __state action pair__. Being in a state and going left has a different value than being in a state and going right. The Q function and delta function look like so: <br>\n",
    "$\\boldsymbol{Q}(\\boldsymbol{s},\\boldsymbol{a}) = \\boldsymbol{r}(\\boldsymbol{s} + 1) + \\gamma \\boldsymbol{r}(\\boldsymbol{s} + 2) \\ldots \\gamma^{\\boldsymbol{n}-1} \\boldsymbol{r}(\\boldsymbol{s}+\\boldsymbol{n})$ <br>\n",
    "$\\delta(\\boldsymbol{s},\\boldsymbol{a}) = (\\boldsymbol{r}(\\boldsymbol{s}+1) + \\gamma \\boldsymbol{Q}(\\boldsymbol{s}+1, \\boldsymbol{a}+1)) - \\boldsymbol{Q}(\\boldsymbol{s}, \\boldsymbol{a})$ <br>\n",
    "\n",
    "The Q Table for a maze of length four would look like the following:\n",
    "<!--\n",
    "##### 4 State:\n",
    "| State: | 1 | 2 | 3 | 4 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Left | 1/2 | 1 | 1/2 | 1/4 |\n",
    "| Right | 1/2 | 1/4 | 1/2 | 1 |\n",
    "-->\n",
    "\n",
    "##### 6 State:\n",
    "| State: | 0 | 1 | 2 | 3 | 4 | 5 |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Left | 0 | 1 | 1/2 | 1/4 | 1/8 | 1/4 |\n",
    "| Right | 0 | 1/4 | 1/8 | 1/4 | 1/2 | 1 |\n",
    "\n",
    "\n",
    "\n",
    "__Warning: This code is not yet functional__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import *\n",
    "init_printing(use_latex=True)\n",
    "from fractions import Fraction\n",
    "\n",
    "EPOCH = 100\n",
    "GAMMA = .5\n",
    "class Maze:\n",
    "    def __init__(self, goal, length):\n",
    "        self.reward = np.zeros(length)\n",
    "        self.value = np.zeros((2,length))\n",
    "        self.goal = goal\n",
    "        self.length = length\n",
    "        \n",
    "        self.reward[goal] = 1\n",
    "        \n",
    "    # Performs one episode of learning on the maze\n",
    "    # S is the starting state\n",
    "    def episode(self, s):\n",
    "         #get action\n",
    "        if (self.Q(s,0) >= self.Q(s,1)):\n",
    "            a = 0\n",
    "        else:\n",
    "            a = 1\n",
    "        while s != self.goal:\n",
    "            self.value[a, s] += self.delta(s,a)\n",
    "            left, right = self.orient(s)\n",
    "            if (a == 0):\n",
    "                s = left\n",
    "            else:\n",
    "                s = right\n",
    "            if (self.Q(s,0) >= self.Q(s,1)):\n",
    "                a = 0\n",
    "            else:\n",
    "                a = 1\n",
    "            \n",
    "    # delta(s,a) = r(s+1) + GAMMA * maxQ(s+1,a+1) - Q(s,a)\n",
    "    def delta(self, s, a):\n",
    "        return  self.Q(s, a) - self.value[a, s]\n",
    "    \n",
    "    # Q(s,a) = r(s+1) + GAMMA * Q(s+1, a+1)\n",
    "    def Q(self, s, a):\n",
    "        # Get next state and action\n",
    "        left, right = self.orient(s)\n",
    "        if (a == 0):\n",
    "            nextS = left\n",
    "        else:\n",
    "            nextS = right\n",
    "        \n",
    "        # Get value\n",
    "        if (s == self.goal):\n",
    "            # no future moves (only reward)\n",
    "            return self.reward[nextS]\n",
    "        else:\n",
    "            if (self.value[0, nextS] >= self.value[1, nextS]):\n",
    "                nextValue = self.value[0, nextS]\n",
    "            else:\n",
    "                nextValue = self.value[1, nextS]\n",
    "            return self.reward[nextS] + GAMMA * nextValue\n",
    "        \n",
    "\n",
    "    def orient(self, s):\n",
    "        # Obtain which state is to the left and which is to the right\n",
    "        if (s == 0):\n",
    "            left = self.length-1\n",
    "            right = s+1\n",
    "        elif (s == self.length-1):\n",
    "            left = s-1\n",
    "            right = 0\n",
    "        else:\n",
    "            left = s-1\n",
    "            right = s+1\n",
    " \n",
    "        return left, right\n",
    "        \n",
    "    \n",
    "    # Print the maze \n",
    "    def display(self):\n",
    "        print(\"The value matrix is:\")\n",
    "        print(\"L: \", end='')\n",
    "        for i in self.value[0]:\n",
    "            print( \"| %3s\" %Fraction(i), end='')\n",
    "        print(\"|\")\n",
    "        print(\"R: \", end='')\n",
    "        for i in self.value[1]:\n",
    "            print( \"| %3s\" %Fraction(i), end='')\n",
    "        print(\"|\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value matrix is:\n",
      "L: |   0|   0|   0|   0|   0|   0|\n",
      "R: |   0|   0|   0|   0|   0|   0|\n",
      "The value matrix is:\n",
      "L: |   0|   1| 1/2| 1/4|   0|   0|\n",
      "R: |   0|   0|   0|   0| 1/2|   1|\n"
     ]
    }
   ],
   "source": [
    "maze = Maze(0, 6)\n",
    "maze.display()\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    s = np.random.randint(0,maze.length)\n",
    "    maze.episode(s)\n",
    "maze.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAASCAYAAAAKRM1zAAAABHNCSVQICAgIfAhkiAAAAnZJREFUWIXt1luIjVEUB/AfjSjJNebBZTS5FXlDFIYyJOX65pJ48OBWlBI13jyJ5gElFOVRXlyTuyelJPc4JKZcMozkejzsferMN993LkjEv3brO2v/1/r22us7ay3+YQzEfjzDR+SwE72r9LMQzbiEt8jjcAa3L1biKB7gA1pxGSvQOcMuF/2mrZZiYk3CsB5X0R/HcAfjsA4zMQmvKggStmAs2vAUI0twF2E3nuMcnmAA5mMfZkVOPsW2VUhEEm2lDncqOluT0O+I+j2ljBNowDB0wlSlMzoNc3TMXK0QdB4LUuxycVWF+ujwUcoLewg39B7dq3WsfKClsDnaNqfs5VQYaPGn2xDlaXxL8N7hCmZgAs5Wfs6fxucov2Tsd8ViDBYScQMX8bWYVBzoiCjvZTi8LwQ63O8LtAZL4/PJDE4tDiV0j7AcFwqK4k+0Z5StGQ4L+l4VH/PnsR2jcVyoH0kcwHQh2O4Yg72owwmhGKJj1f2TsBYbhMq/JIOzLfH7JlYJ9WQDmjCP9hktZKyndBT0b6o67o9hNXbhllA7XldpX+gOkwuK4kDvRjk8w3hYlFn/4V+F9UKFvSkE2VKanooXUaZ2iD+hvWyKvOvo9wPvKaAx+rmVRah2YKgXJp4uZV48VflAt0bONfQp4w9GSb/0OqFD5IUejDC1FCM5At7GeOETuoeJ2o+AOQzBUB0b99y4CFWxEQ+F2RdeYmN8XoaDQu9rll75c5FTQJNQcC7isdDr6zEb3YRKPQ+fUnyBQULZfh5Jj2UP9Tnh5upS9ppkD9x57S+mHDeP8wn/U3BEqMpvhMHiBc4IvTeZxP/4j78R3wH32a3FuNuh1wAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$$0.125$$"
      ],
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze.delta(4,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
